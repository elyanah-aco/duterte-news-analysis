---
title: "duterte_news_analysis"
output: html_notebook
---

## Libraries

```{r}
library(tidyverse)
library(tidytext)
library(lubridate)
library(stringr)
library(ggplot2)
library(wordcloud2)
library(ldatuning)
library(topicmodels)
```

## Data Compilation

Facebook captions from Sept. 2016 to June 2021 were obtained from nine Filipino news outlets.

```{r}
## Data Compilation

fnames <- list.files(path = "Datasets")
compiled <- tibble()
for(i in fnames){
  doc <- read.csv(paste0("Datasets/",i),
                  stringsAsFactors =  FALSE,
                  sep = ";",
                  encoding = "UTF-8")
  doc <- doc %>%
    select(message, from.name, created_time) %>%
    filter(message != "") %>%                 
    ## Remove rows with blank (not NA) messages
    mutate(created_time = ymd_hms(created_time),
           year = year(created_time),
           month = month(created_time, label = TRUE)) %>%
    drop_na(created_time) %>%                
    ## Remove rows where created_time did not format properly (due to errors in FacePager)
    distinct()
  compiled <- bind_rows(doc,compiled)
}
```

## Data Cleaning: All news on Duterte

First, we want to determine if news outlets reported more or less about Duterte's drug war throughout the years. 


```{r}
################################
## Stripping words
################################

remove_expressions <- "&amp;|&lt;|&gt;|\\s+|http\\S+|www\\S+|bit.ly\\S+|[[:punct:]]|\\p{So}|\\p{Cn}|\\p{Sc}|\\p{S}|[^x01-\x7F]"
stripped_words <- compiled %>% 
  mutate(stripped_text = tolower(str_replace_all(compiled$message, remove_expressions, " "))) %>%
  mutate(message = stripped_text) %>%
  filter(grepl("duterte|digong|rodrigo|rody|president|pangulo",message))

################################
## Adding stopwords
################################

tl_stopwords_addtl <- c("ago", "ah", "akala", "akong", "andyan", "anong", "ayan", "ayaw",
                   "ba", "balang", "bale", "bang", "basta", "daw", "de", "del",
                   "di", "diyan", "dun", "eh", "ewan", "finally", "ganito", "ganon", 
                   "ganoon", "ganun", "gaya", "ha", "iba", "ibang", "ika", "ilang", 
                   "inyo", "itong", "iyan", "iyang", "kanina", "kami", "kaming", "kasi", 
                   "kay", "kayo", "kayong", "ke", "kuwan","lalo", "lang", "mag", "magho", 
                   "maka", "mas", "mi", "mo", "naano", "nag", "nagma","nagsa",
                   "nai", "naku", "natin", "naman", "namang", "nandoon","nang", "nangya", 
                   "nga", "ninyo", "niyan", "nyan", "niyo", "noong", "nung", "oo", "pag", 
                   "pagka", "pala", "pang", "parang", "pati", "pong", "puro", "rin", 
                   "sabagay" ,"saka", "sakali", "sakaling", "si", "siya", "siyang", "sus", 
                   "taga", "talaga", "tayo", "tayong", "uli", "yan", "yang", "yo", "yun", 
                   "yung")
new_stopwords <- tibble(
  word = tl_stopwords_addtl,
  lexicon = "addtl_tl"
)
stopwords_tl <- read_csv("stopwords_tl.csv")
stopwords_merged <- rbind(stopwords_tl, stop_words, new_stopwords)
```


## Topic Modelling: All news on Duterte

First, we determine whether news outlets have reported more or less about Duterte's war on drugs throughout the years. We include all spiels that mention Duterte (or variations thereof), create topic models, and determine yearly changes in  specific topic/s on war on drugs (if any). For this, we consider all relevant captions made by one outlet for a year as one document. For example, "CNN Philippines_2017", or the set of captions made by CNN Philippines in 2017, is a different document from "CNN Philippines_2016" or "Rappler_2017".

We remove words below the mean frequency per word or are only at most two characters long to reduce computational power.

Since we are not fully cleaning the data for topic modelling, we expect topics to contain words that have no meaning but are used frequently, such as brand names. We leave them be for this stage since we are only interested in identifying topics about the war on drugs.

```{r}

################################
## Preliminary Cleaning
################################

compiled_words_freq_prelim <- stripped_words %>%
  mutate(document = paste0(from.name,"_",year)) %>%
  unnest_tokens(word, stripped_text) %>%
  anti_join(stopwords_merged, by = "word") %>%
  mutate(word = str_extract(word, "[a-z]+")) %>%  
  drop_na(word) %>%
  anti_join(stopwords_merged, by = "word") %>%
  count(document, word, name = "freq", sort = TRUE)

mean(compiled_words_freq_prelim$freq) ## The mean is 2.45. We remove words that show up in a document at most 3 times.

################################
## Creating DTM Object
################################

compiled_dtm_prelim <- compiled_words_freq_prelim %>%
  mutate(nchar = nchar(word)) %>%
  filter(freq > 3) %>% 
  filter(nchar >= 3) %>% 
  ## Some two-letter "words" are "remnants" of data cleaning.
  cast_dtm(document, word, freq)

################################
## Finetuning Controls
################################

control_list_gibbs <- list(
  burnin = 500,
  iter = 1000,
  seed = 0:10,
  nstart = 11,
  best = TRUE
)


###################################################
## Determining best model using log-likehood
###################################################


mod_log_prelim = numeric(25)  
for(i in 2:25){
  mod = LDA(x = compiled_dtm_prelim,
            k = i,
            method = "Gibbs",
            control = control_list_gibbs)
  mod_log_prelim[i] = logLik(mod)
}

tibble(k = 2:25, score = mod_log_prelim[-1]) %>%
  top_n(3, score) %>%
  arrange(-score) ## Result : 24 topics

###################################################
## Creating LDA model
###################################################

lda_model_prelim <- LDA(x = compiled_dtm_prelim,
                     k = 24,
                     method = "Gibbs",
                     control = control_list_gibbs)

lda_beta_prelim <- tidy(lda_model_prelim, matrix = "beta")

lda_beta_prelim %>%
  filter(topic == 1) %>%  ## Change the number to see the words for each topic
  select(word = term, freq = beta) %>%
  top_n(15,freq) %>%
  wordcloud2(
    size = .5, 
    gridSize = 10,
    shape = "diamond",
    color = "random-dark"
  )
```

We identify Topics 16 and 24 to be possibly related to war on drugs. Topic 16 is not completely about war on drugs in general, but words such as "war", drugs", "leila [de lima]", "death" and "probe" are in the top 30. Topic 24 is more clearly about the topic; words such as "drugs", "killings", "war", "police", "matobato" and "extrajudicial" are in the top 30.

```{r}
###################################################
## Yearly trends on topics
###################################################

lda_gamma_prelim <- tidy(lda_model_prelim, matrix = "gamma")

prelim_outlet_per_year <- lda_gamma_prelim %>%
  separate(document, c("from.name","year"), sep = "_") %>%
  select(from.name,year,topic,gamma) %>%
  filter(topic == 16 | topic == 24) %>%
  arrange(year) %>%
  mutate(gamma = round(gamma,4)) %>%
  pivot_wider(names_from = year, values_from = gamma) %>%
  arrange(topic, from.name)

prelim_topics_per_year <- lda_gamma_prelim %>% 
  separate(document, c("from.name","year"), sep = "_") %>% 
  group_by(topic, year) %>% 
  summarize(sum_gamma = sum(gamma)) %>% 
  ungroup() %>% 
  group_by(year) %>% 
  mutate(prop = round(sum_gamma / sum(sum_gamma), %>%
  select(-sum_gamma) %>%
  filter(topic == 16 | topic == 24) %>%
  arrange(year) %>%
  pivot_wider(names_from = year, values_from = prop) %>%
  arrange(topic)

```

Note that gamma is an estimated proportion of words from a given document that are generated from the topic. In the drug_topic_per_year example, the LDA estimates that about 6% of the words used by ABS-CBN in 2016 were from Topic 3.

Although the proportions given are just estimates and should not be taken as completely accurate, they can still be used to identify trends. 


## Data cleaning : Duterte and war on drugs

Now, we focus on news captions particularly about Duterte and drugs. We only obtain captions that both contain "Duterte" (or variations thereof) and "drugs" (or variations thereof). We also remove messages with words related to the COVID-19 pandemic, since these may discuss medical drugs.

```{r}
stripped_words <- compiled %>% 
  mutate(stripped_text = tolower(str_replace_all(compiled$message, remove_expressions, " "))) %>%
  mutate(message = stripped_text) %>%
  filter(grepl("duterte|digong|rodrigo|rody|president|pangulo",message)) %>%
  filter(grepl("droga|drug|shabu",message)) %>%
  filter(!grepl("covid|pandemic|pandemya|vaccin",message))

compiled_words <- stripped_words %>%
  unnest_tokens(output = word,
                input = stripped_text) %>%
  anti_join(stopwords_merged, by = "word") %>%
  mutate(word = str_extract(word, "[a-z]+")) %>%  
  drop_na(word) %>%
  anti_join(stopwords_merged, by = "word") 
```

## Determining other stopwords

In order to further refine our topic model, we need to also remove the following words or phrases usually used in captions. Otherwise, the model may identify topics revolving around these words just because how often they are used.

   1) Brand words (eg. "snstr" for SunStar, or "ThewRap" for Rappler)
   2) Unique patterns or phrases used in reporting (For example, some of Philippine Star's news started with "Today's front news", while some in Rappler's ended with "Full story")
   3) Names of reporters
   
Although it is manually intensive to get an exhaustive list of the above, we expect that these words are unique for a given outlet. We can determine the top 10 words per news outlet based on TF-IDF scores.


```{r}
################################
## TF-IDF - Per News Outlet
################################

compiled_freq_news <- compiled_words %>%
  count(word, from.name, name = "freq") %>%
  group_by(from.name) %>%
  mutate(proportion = round(freq/sum(freq),5)) %>%
  arrange(from.name, desc(proportion)) %>%
  ungroup()

compiled_tf_idf_news <- compiled_freq_news %>%
  group_by(from.name, word) %>%
  summarize(n = sum(freq)) %>%
  bind_tf_idf(word, from.name, n) %>%
  arrange(from.name, desc(tf_idf)) %>%
  ungroup()

compiled_tf_idf_news %>% 
  group_by(from.name) %>%
  head(10)
```



By looking at the words with high TF-IDF scores and determining how they are used in news captions, we identify the following words that also need to be removed:

  1) GMA News: gma, jessica, soho, kapuso
  2) MANILA BULLETIN: bulletin, befullyinformed, manilabulletin, headline, instagram, newsstand, copy, follow, twitter, grab, website, favorite, mbrundown
  3) Rappler: rappler, thewrap, story
  4) CNN Philippines: pia, cnn, cnnphnewsnight, highlights, hontiveros, headlines, happening, tonight, lxc
  5) INQUIRER.net: inqnews, inquirer, icymi, julliane, net, red
  6) Philippine Star: thephilippinestar, page, front, charles, cignaltv, lejano, weareonenews, newsroom, onenewsroom, december, september, thursday, friday, tuesday
  7) SunStar Philippines: snstr, sunstarpilipinas, facebook, subscribe
  8) Multiple: breaking, youtube, ruth



```{r}
################################
## Stopwords per News Outlet
################################

outlet_stopwords <- tibble(word = c("gma","jessica","soho","kapuso","bulletin", "befullyinformed", "manilabulletin", "headline", "instagram", "newsstand", "copy", "follow", "grab","twitter", "website","favorite","mbrundown","rappler", "story", "thewrap", "pia","cnn","cnnphnewsnight","highlights","hontiveros","headlines","happening", "tonight", "lxc", "inqnews", "inquirer", "icymi", "julliane", "net","read","thephilippinestar","page","front","charles","cignaltv","lejano","weareonenews","newsroom","onenewsroom","december","september", "thursday", "friday", "tuesday","snstr", "sunstarpilipinas", "facebook","subscribe","breaking", "youtube", "ruth"),
                         lexicon = "news_stopwords")

stopwords_merged_new <- rbind(stopwords_tl, stop_words, new_stopwords, outlet_stopwords)

```



## Topic Modelling: Duterte and war on drugs

We perform topic modelling to identify topics in our corpus. Just like the earlier topic model, we consider all relevant captions made by one outlet for a year as one document. 

```{r}

################################
## Creating DTM object
################################

compiled_dtm <- stripped_words %>%
  mutate(document = paste0(from.name,"_",year)) %>%
  unnest_tokens(word, stripped_text) %>%
  anti_join(stopwords_merged_new, by = "word") %>%
  mutate(word = str_extract(word, "[a-z]+")) %>%  
  drop_na(word) %>%
  anti_join(stopwords_merged_new, by = "word") %>%
  count(document, word, name = "freq", sort = TRUE) %>%
  mutate(nchar = nchar(word)) %>%
  filter(nchar > 2) %>%
  cast_dtm(document, word, freq)
  

################################
## Finetuning Controls
################################

control_list_gibbs <- list(
  burnin = 500,
  iter = 1000,
  seed = 0:10,
  nstart = 11,
  best = TRUE
)

###################################################
## Determining best model using log-likehood
###################################################

mod_log_lik = numeric(25)
for(i in 2:25){
  mod = LDA(x = compiled_dtm,
            k = i,
            method = "Gibbs",
            control = control_list_gibbs)
  mod_log_lik[i] = logLik(mod)
}

tibble(k = 2:25, score = mod_log_lik[-1]) %>%
  top_n(3, score) %>%
  arrange(-score) ## Result : 20 topics


###################################################
## Creating LDA model
###################################################


lda_model_log <- LDA(x = compiled_dtm,
                     k = 20,
                     method = "Gibbs",
                     control = control_list_gibbs)


lda_beta_log <- tidy(lda_model_log, matrix = "beta")

lda_beta_log %>%
  filter(topic == 4) %>% ## Change topic number to see words per table
  select(word = term, freq = beta) %>%
  arrange(desc(freq)) %>%
  print(n = 15) %>%
  wordcloud2(
    size = .5, 
    gridSize = 10,
    shape = "diamond",
    color = "random-dark"
  )
```


We get the mean gammas of each topic to determine which specific topics to further discuss. The higher the mean, the more often it is being reported by outlets.

```{r}
lda_gamma_log %>% group_by(topic) %>% summarize(mean = mean(gamma)) %>% arrange(desc(mean))
```

The top 5 topics are (in decreasing order of means):

  1) Topic 12: War on drugs in general
  2) Topic 1: Duterte and martial law
  3) Topic 3: Duterte's anti-drug campaign during 2016 elections
  4) Topic 20: Leni as advocate against war on drugs
  5) Topic 5: ICC investigation on war on drugs
  
We visualize how topics have changed by year, emphasizing the five topics above.

```{r}

###################################################
## Visualizing trends per year
###################################################

lda_gamma_log <- tidy(lda_model_log, matrix = "gamma")

lda_gamma_log %>%
  separate(document, c("from.name","year"), sep = "_") %>%
  select(from.name,year,topic,gamma) %>%
  filter(from.name == "GMA News") %>%
  filter(topic == 1) %>%
  arrange(year) %>%
  group_by(topic) %>%
  top_n(1,gamma) %>%
  ungroup()

wod_topics_per_year <- lda_gamma_log %>% 
  separate(document, c("from.name","year"), sep = "_") %>% 
  group_by(topic, year) %>% 
  summarize(sum_gamma = sum(gamma)) %>% 
  ungroup() %>% 
  group_by(year) %>% 
  mutate(prop = sum_gamma / sum(sum_gamma),
         topic_name = ifelse(topic == 12, "War on drugs",
                             ifelse(topic == 1, "Duterte and martial law",
                                    ifelse(topic == 3, "Anti-drug campaign during 2016 elections",
                                           ifelse(topic == 20, "Leni as advocate against war on drugs",
                                                  ifelse(topic == 5, "ICC investigation on war on drugs", "Other topics"))))))

ggplot(wod_topics_per_year, aes(x = year, y = prop, fill = factor(topic_name, levels = c("Other topics","ICC investigation on war on drugs","Leni as advocate against war on drugs","Anti-drug campaign during 2016 elections", "Duterte and martial law", "War on drugs")))) + 
  geom_bar(stat="identity") + ylab("proportion") +
  scale_fill_manual("Topic", values = c("Other topics" = "#e8e8e8", 
                                        "ICC investigation on war on drugs" = "#2c7bb6", 
                                        "Leni as advocate against war on drugs" = "#abd9e9",                                             "Anti-drug campaign during 2016 elections" = "#ffffbf", 
                                        "Duterte and martial law" = "#fdae61",
                                        "War on drugs" = "#d7191c")) + 
  labs(x = "Year",
       y = "Proportion") + 
  theme_bw()
```

